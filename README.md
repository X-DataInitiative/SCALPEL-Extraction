[![CircleCI](https://circleci.com/gh/X-DataInitiative/SNIIRAM-featuring.svg?style=shield&circle-token=6dd3a84c5ec9b9d3acac9e1ed156077eeadf9780)](https://circleci.com/gh/X-DataInitiative/SNIIRAM-featuring)
[![codecov](https://codecov.io/gh/X-DataInitiative/SNIIRAM-featuring/branch/master/graph/badge.svg?token=4a0h501t8P)](https://codecov.io/gh/X-DataInitiative/SNIIRAM-featuring)

This repository contains the ETL (Extract, Transform & Load) and Study specific code of the processing pipeline for the SNIIRAM pharmacovigilance project in partnership with CNAMTS.

Please read our **[code conventions](https://datainitiative.atlassian.net/wiki/display/CFC/Development#)** before contributing to this repository.

# ETL stage

The ETL process of this project contains the code responsible for reading the flat data generated by the Flattening package and creating the features for the models. This code contains 4 main steps:
                                
1) Reading the flat data from the parquet files generated by the flattening step;
2) Extracting "raw" events (e.g. Drug Dispensations, Diagnoses, Medical Acts, etc) with a normalized schema;
3) Transforming the "raw" events into "processed" events (follow-up periods, molecule exposures, outcomes, etc), also with a normalized schema;
4) Loading the processed events into output files in the form of features for the models we want to apply. 

A better description of the pipeline can be found in [Confluence](https://datainitiative.atlassian.net/wiki/spaces/CFC/pages/204963841/ETL).

# Configuration

For each study, a template configuration file is defined containing all the overridable values. When
running the job, if a configuration item is to be changed, one needs to copy this template file to 
the server, then edit it uncommenting and changing the appropriate lines. Finally, the path to this 
new file is passed to spark-submit.

For example, the template configuration file for the Pioglitazone study is defined 
[here](src/main/resources/config/pioglitazone/template.conf). So, if one wants to override 
`min_purchases`, `purchases_window` and `cancer_definition`, they just need to create a copy of this file on the master 
 server and uncomment these lines changing the appropriate values:

```hocon
# Previous line stay commented...

exposures.min_purchases: 2               // 1+ (Usually 1 or 2)
# exposures.start_delay: 0               // 0+ (Usually between 0 and 3). Represents the delay in months between a dispensation and its exposure start date.
exposures.purchases_window: "6 months"   // 0+ (Usually 0 or 6) Represents the window size in months. Ignored when min_purchases=1.

outcomes.cancer_definition: "narrow"   // "naive" | "broad" | "narrow"

# Next lines stay commented...
```

This file should then be stored with the results, so it would be easy to see which parameters were
changed.

# Execution

The entry points for executing the pipeline are study-specific, therefore within the `study` package.
The documentation for the studies can be found in Confluence at [this page](https://datainitiative.atlassian.net/wiki/spaces/CFC/pages/216137729/Studies). 

For executing the pipeline for a given study, usually one uses the following steps:

1. Clone the code into a master node (usually `master03` or `master04`) within the home directory.
2. Copy the template configuration file for the given study into the same master node and edit it.
3. Checkout the tag or branch from which one wants to execute the pipeline.
4. Build the jar with the command `sbt assembly` (ran at the root of the project)
5. Execute the code with a `spark-submit` command containing:
    - `--total-executor-cores` and `--executor-memory` arguments;
    - any wanted `--conf` commands for Spark parameters;
    - `--class` argument pointing to the study's main class;
    - the path to the jar file created by `sbt assembly`;
    - the job arguments: path to config file and environment name.

For the 5th step, one can create an alias or script to make things easier. For example, for the 
Pioglitazone study, one could run the following shell script:

```bash
#!/bin/sh
spark-submit \
  --driver-memory 40G \
  --executor-memory 110G \
  --total-executor-cores 150 \
  --conf spark.task.maxFailures=20 \
  --class fr.polytechnique.cmap.cnam.study.pioglitazone.PioglitazoneMain \
  ./target/scala-2.11/SNIIRAM-featuring-assembly-1.0.jar conf=./overrides.conf env=cnam
```
